{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will create an image classifier to detect playing cards.\n",
    "We will tackle this problem in 3 parts:\n",
    "\n",
    "1. Pytorch Dataset\n",
    "2. Pytorch Model\n",
    "3. Pytorch Training Loop\n",
    "\n",
    "Almost every pytorch model training pipeline meets this paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing essential libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim #for optimizer\n",
    "from torch.utils.data import Dataset, DataLoader #for data preprocessing and loading\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder #for loading data\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Pytorch Dataset (and Dataloader)\n",
    "\n",
    "For training any model in pytorch we need to set up our dataset first. \n",
    "Datasets are important: \n",
    "    .It's an organized way to structure how the data and labels are loaded into the model.\n",
    "    .We can then wrap the dataset in a dataloader and pytorch will handle batching the shuffling the data for us when training the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will define a class for our cards dataset in the following fashion in pytorch\n",
    "class PlayingCardDataset(Dataset): ##our class inherits methods from Dataset class of pytorch\n",
    "    def __init__(self, data_dir, transform=None): ##we will intialize our object with the data_dir path of our dataset\n",
    "        self.data = ImageFolder(data_dir, transform=transform) ##data is loaded using ImageFolder library we imported above\n",
    "    \n",
    "    def __len__(self):   ##default method in pytorch to define the total length of data samples in a dataset\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):  ##returns a sample from our dataset using idx\n",
    "        return self.data[idx]\n",
    "    \n",
    "    @property\n",
    "    def classes(self):  ##additional method to help us to know the data classes in our dataset\n",
    "        return self.data.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=PlayingCardDataset(data_dir='D:/education/pyhton/projects/datasets/Cards_Image_Dataset_Classification/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if dataset is properly loaded \n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking by loading a sample from dataset\n",
    "image, label = dataset[5000]\n",
    "print(label)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dictionary associating target values with folder names\n",
    "data_dir = 'D:/education/pyhton/projects/datasets/Cards_Image_Dataset_Classification/train'\n",
    "target_to_class = {v: k for k, v in ImageFolder(data_dir).class_to_idx.items()}\n",
    "print(target_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Applying the data preprocessing steps on the loaded dataset like basic transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data_dir = data_dir\n",
    "dataset = PlayingCardDataset(data_dir, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the applied transformation\n",
    "image, label = dataset[100]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over dataset\n",
    "for image, label in dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader\n",
    "\n",
    "    1. used to batch our dataset automatically using Dataloader in pytorch\n",
    "    2. Faster parallel processing of data in batches and therefore faster to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laoding dataset into a dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##as we used shuffle above while loading data, data is organized randomly like below.\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Pytorch ModelÂ¶\n",
    "\n",
    "Pytorch datasets have a structured way of organizing your data, pytorch models follow a similar paradigm.\n",
    "\n",
    "We could create the model from scratch defining each layer.\n",
    "However for tasks like image classification, many of the state of the art architectures are readily available and we can import them from packages like timm.\n",
    "Understanding the pytorch model is all about understanding the shape the data is at each layer, and the main one we need to modify for a task is the final layer. Here we have 53 targets, so we will modify the last layer for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardClassifier(nn.Module):\n",
    "    def __init__(self,num_classes=53):\n",
    "        super(CardClassifier,self).__init__() ##initializing object of parent class nn.Module\n",
    "        ##we define here all the parts of the model like layers, base model, any alterations to the model layers and so on\n",
    "        self.base_model= timm.create_model('efficientnet_b0',pretrained=True) ##we take here the pretrained model of efficientnet that is pretrained on Imagenet\n",
    "        self.features=nn.Sequential(*list(self.base_model.children())[:-1]) ###altering the last layer of base model as our task has only 53 classes for classification where as base model is huge with different number of classes for classification.\n",
    "\n",
    "        #output_size of enet\n",
    "        enet_out_size = 1280\n",
    "        ##Make a classifier for our task\n",
    "        self.classifier = nn.Sequential(   \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(enet_out_size, num_classes)\n",
    "        ) ###last layer is altered with these lines\n",
    "\n",
    "    def forward(self,x):\n",
    "        ##connect the above defined parts of model to the input and perform a forward pass\n",
    "        x=self.features(x)\n",
    "        ouput=self.classifier(x)\n",
    "        return ouput    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##defining model for our task with our input\n",
    "model=CardClassifier(num_classes=53) ##model object is instantiated with the above defined class\n",
    "print(str(model)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check for correctness of model created\n",
    "example_out = model(images)\n",
    "example_out.shape # [batch_size, num_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. The training loop\n",
    "\n",
    "Now that we understand the general paradigm of pytorch datasets and models, we need to create the process of training this model.\n",
    "Some things to consider:\n",
    "    \n",
    "    We want to validate our model on data it has not been trained on, so usually we split our data into a train and validate datasets. This is easy because we can just create two datasets using our existing class.\n",
    "\n",
    "Terms:\n",
    "   \n",
    "    Epoch: One run through the entire training dataset.\n",
    "    Step: One batch of data as defined in our dataloader\n",
    "\n",
    "This loop is one you will become familiar with when training models, you load in data to the model in batches - then calculate the loss and perform backpropagation. There are packages that package this for you, but it's good to have at least written it once to understand how it works.\n",
    "\n",
    "Two things to select:\n",
    "\n",
    "    optimizer, adam is the best place to start for most tasks.\n",
    "    loss function: What the model will optimize for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check for loss_fn working\n",
    "criterion(example_out, labels) ##loss between predictions and actual GT\n",
    "print(example_out.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_folder = 'D:/education/pyhton/projects/datasets/Cards_Image_Dataset_Classification/train/'\n",
    "valid_folder = 'D:/education/pyhton/projects/datasets/Cards_Image_Dataset_Classification/valid/'\n",
    "test_folder = 'D:/education/pyhton/projects/datasets/Cards_Image_Dataset_Classification/test/'\n",
    "\n",
    "train_dataset = PlayingCardDataset(train_folder, transform=transform)\n",
    "val_dataset = PlayingCardDataset(valid_folder, transform=transform)\n",
    "test_dataset = PlayingCardDataset(test_folder, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 ##defining the number of epochs\n",
    "train_losses, val_losses = [], [] ##empty lists for storing the losses\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") ##move the training onto GPU for fastness.\n",
    "#model definition\n",
    "model = CardClassifier(num_classes=53)\n",
    "model.to(device) #send the training to GPU\n",
    "#loss and optimizer definition\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    #for images, labels in tqdm(train_loader, desc='Training loop'): ##added a progress bar using tqdm module\n",
    "    for images, labels in train_loader:    \n",
    "        # Move inputs and labels to the device\n",
    "        images, labels = images.to(device), labels.to(device) ##input and output are loaded onto GPU\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        #for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
    "        for images, labels in val_loader:    \n",
    "            # Move inputs and labels to the device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "         \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Losses\n",
    "\n",
    "We can plot our training and validation loss through this training, usually we do this at the end of each epoch. We see that our accuracy on the validation dataset is x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the image\n",
    "def preprocess_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image, transform(image).unsqueeze(0)\n",
    "\n",
    "# Predict using the model\n",
    "def predict(model, image_tensor, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    return probabilities.cpu().numpy().flatten()\n",
    "\n",
    "# Visualization\n",
    "def visualize_predictions(original_image, probabilities, class_names):\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    \n",
    "    # Display image\n",
    "    axarr[0].imshow(original_image)\n",
    "    axarr[0].axis(\"off\")\n",
    "    \n",
    "    # Display predictions\n",
    "    axarr[1].barh(class_names, probabilities)\n",
    "    axarr[1].set_xlabel(\"Probability\")\n",
    "    axarr[1].set_title(\"Class Predictions\")\n",
    "    axarr[1].set_xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "test_image = \"D:/education/pyhton/projects/datasets/Cards_Image_Dataset_Classification/test/five of diamonds/2.jpg\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "original_image, image_tensor = preprocess_image(test_image, transform)\n",
    "probabilities = predict(model, image_tensor, device)\n",
    "\n",
    "# Assuming dataset.classes gives the class names\n",
    "class_names = dataset.classes \n",
    "visualize_predictions(original_image, probabilities, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predicting on 10 sample test images\n",
    "from glob import glob\n",
    "test_images = glob('D:/education/pyhton/projects/datasets/Cards_Image_Dataset_Classification/test/*/*')\n",
    "test_examples = np.random.choice(test_images, 10)\n",
    "\n",
    "for example in test_examples:\n",
    "    original_image, image_tensor = preprocess_image(example, transform)\n",
    "    probabilities = predict(model, image_tensor, device)\n",
    "\n",
    "    # Assuming dataset.classes gives the class names\n",
    "    class_names = dataset.classes \n",
    "    visualize_predictions(original_image, probabilities, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mspec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
